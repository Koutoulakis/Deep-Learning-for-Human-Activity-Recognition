DNN configurations needed to achieve f1_score of 0.639072634384 for Daphnet Gait dataset

1)Downsampled at 30hz. (the dataset)

2)50% overlap in the sliding windows.

3)Parameters:
window size = 25 (creates approximately 470k samples)
labels = 2
features/measurements = 9
batch size = 64
learning rate = 0.001
training epochs = 500

4)Model:
2 hidden fully connected layers of 256 neurons each.
Each layer passes through ReLU activation function. 
The dropout rate is 0.3 for the first layer and 0.5 for the rest layers
during training.
Max pooling is applied after the last layer.

5)Optimization function:
Adam Optimizer, Gradient Descent returned worse
f1 scores (around 8% lower than Adam, it converges much slower)

6)Results:

f1_score : 0.639072634384
confusion_matrix
[[3192   98]
 [ 361  121]]

Time spent in a quad core, i5 pentium 2.8Ghz and 12GB RAM is 1027.11577797 seconds

